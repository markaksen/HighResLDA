{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "import preprocessing # import Jupyter notebook\n",
    "#from preprocessing import clean_pdf\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_keep(meta_df,fields):\n",
    "    pass\n",
    "def clean_pdf(text_df, file_name='', output_dir='',section_lvl = False):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    # if index is not paper_id\n",
    "    if text_df.index.name is None: \n",
    "        print('changing index to paper_id')\n",
    "        text_df = text_df.set_index('paper_id')\n",
    "\n",
    "    ids = text_df.index.values.astype(str)\n",
    "        \n",
    "    contents = text_df['whole_text'].values.tolist()\n",
    "    \n",
    "    # Add abstract to text\n",
    "    if not section_lvl:\n",
    "        abstracts = text_df['abstract'].values.tolist()\n",
    "\n",
    "        contents = [i + j for i, j in zip(contents, abstracts)]\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    #print(contents)\n",
    "    \n",
    "    contents = map(lambda x: re.sub('\\s+', ' ', x), contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: lower case text\n",
    "    contents = map(lambda x: x.lower(),contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: keep alphanumeric\n",
    "    contents = map(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x), contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stand along numbers\n",
    "    contents = map(lambda x: re.sub(\" \\d+ \", \" \", x), contents)\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stop words\n",
    "    contents = map(remove_stopwords, contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    contents = list(contents)\n",
    "\n",
    "    # Preprocessing: remove short text\n",
    "    inds = find_longer_text(contents)\n",
    "    contents = itertools.compress(contents, inds)\n",
    "    ids = list(itertools.compress(ids, inds))\n",
    "    \n",
    "    key_words = text_df.loc[ids]['key_words'].values\n",
    "    print('Tokenizing')\n",
    "    \n",
    "    # Tokenize words + remove punctuation\n",
    "    tokenized_contents = map(tokenize,contents) # documents in BOW format\n",
    "#     word_list = [tokenize(article) for article in contents]\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove numbers\n",
    "    tokenized_contents = map(remove_digits, tokenized_contents)\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Keep longer words\n",
    "#     word_list = [keep_longer_words(words) for words in  word_list]\n",
    "    tokenized_contents = map(keep_longer_words,  tokenized_contents)\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Lemmatizing')\n",
    "    \n",
    "    # Preprocessing: lemmatize\n",
    "    tokenized_contents = map(lemmatize, tokenized_contents)\n",
    "    \n",
    "#     print(list(word_list)[0])\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Bag of Words Representation')\n",
    "    tokenized_contents = list(tokenized_contents)\n",
    "\n",
    "    dct = corpora.Dictionary(tokenized_contents) # make dct before corpus\n",
    "#     doc2bow = partial(dct.doc2bow,allow_update=True)\n",
    "    \n",
    "    print('length of dct before filter_extreme: ', len(dct))\n",
    "    dct.filter_extremes() # using default params # filter dct before creating corpus\n",
    "    print('length of dct after filter_extreme: ', len(dct))\n",
    "    \n",
    "\n",
    "    # Make corpus after any changes to dct\n",
    "    corpus = list(map(lambda x: dct.doc2bow(x,allow_update=True), tokenized_contents))    \n",
    "#     corpus = [dct.doc2bow(doc, allow_update=True) for doc in word_list]\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "\n",
    "\n",
    "    \n",
    "    word_list =  [item for sublist in tokenized_contents for item in sublist]\n",
    "    counter=collections.Counter(word_list)\n",
    "    \n",
    "    d = {'dct': dct, 'corpus': corpus, 'docs': tokenized_contents, \n",
    "                 'counter': counter, 'ids': ids, 'word_list':word_list,\n",
    "                 'key_words':key_words}\n",
    "                 \n",
    "    if file_name:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file_name = output_dir + file_name + '_clean.pkl'\n",
    "\n",
    "\n",
    "        with open(output_file_name, 'wb') as f:  # Python 3: open(..., 'wb') \n",
    "            pickle.dump(d, f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "with open('./Dataset/merged/textdata_all.pkl', 'rb') as f:\n",
    "    text_df = pickle.load(f)\n",
    "text_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>key_words</th>\n",
       "      <th>body_text</th>\n",
       "      <th>whole_text</th>\n",
       "      <th>citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18980380</td>\n",
       "      <td>This technical note studies Markov decision pr...</td>\n",
       "      <td>[Distributional robustness,  Markov decision p...</td>\n",
       "      <td>[{'section': 'II. PRELIMINARIES', 'text': 'Thr...</td>\n",
       "      <td>Throughout the technical note, we use capital ...</td>\n",
       "      <td>{'BIBREF0': {'title': 'Distributionally robust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56031008</td>\n",
       "      <td>Abstract-The present study attempted to find o...</td>\n",
       "      <td>[listening comprehension,  pre-task activities...</td>\n",
       "      <td>[{'section': 'A. Listening Materials and Activ...</td>\n",
       "      <td>Morley (1991) has explained that in developing...</td>\n",
       "      <td>{'BIBREF0': {'title': 'Listening', 'authors': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88484504</td>\n",
       "      <td>In this paper, we address robust design of sym...</td>\n",
       "      <td>[Downlink MU-MISO,  imperfect CSI,  symbolleve...</td>\n",
       "      <td>[{'section': 'II. SYSTEM AND UNCERTAINTY MODEL...</td>\n",
       "      <td>We consider an MU-MISO wireless broadcast chan...</td>\n",
       "      <td>{'BIBREF0': {'title': 'Convex optimization-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88485902</td>\n",
       "      <td>ABSTRACT A kinematic equation of profiling flo...</td>\n",
       "      <td>[Profiling float,  depth control,  low power c...</td>\n",
       "      <td>[{'section': 'I. INTRODUCTION', 'text': 'With ...</td>\n",
       "      <td>With the increase in the cognition of marine a...</td>\n",
       "      <td>{'BIBREF0': {'title': 'AUV buoyancy regulating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204197524</td>\n",
       "      <td>We characterize practical optical signal recei...</td>\n",
       "      <td>[Optical wireless communications,  multi-stage...</td>\n",
       "      <td>[{'section': 'A. PMT Principle Review', 'text'...</td>\n",
       "      <td>The typical structure of a PMT is shown in Fig...</td>\n",
       "      <td>{'BIBREF0': {'title': 'A statistical non-linea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id                                           abstract  \\\n",
       "0   18980380  This technical note studies Markov decision pr...   \n",
       "1   56031008  Abstract-The present study attempted to find o...   \n",
       "2   88484504  In this paper, we address robust design of sym...   \n",
       "3   88485902  ABSTRACT A kinematic equation of profiling flo...   \n",
       "4  204197524  We characterize practical optical signal recei...   \n",
       "\n",
       "                                           key_words  \\\n",
       "0  [Distributional robustness,  Markov decision p...   \n",
       "1  [listening comprehension,  pre-task activities...   \n",
       "2  [Downlink MU-MISO,  imperfect CSI,  symbolleve...   \n",
       "3  [Profiling float,  depth control,  low power c...   \n",
       "4  [Optical wireless communications,  multi-stage...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  [{'section': 'II. PRELIMINARIES', 'text': 'Thr...   \n",
       "1  [{'section': 'A. Listening Materials and Activ...   \n",
       "2  [{'section': 'II. SYSTEM AND UNCERTAINTY MODEL...   \n",
       "3  [{'section': 'I. INTRODUCTION', 'text': 'With ...   \n",
       "4  [{'section': 'A. PMT Principle Review', 'text'...   \n",
       "\n",
       "                                          whole_text  \\\n",
       "0  Throughout the technical note, we use capital ...   \n",
       "1  Morley (1991) has explained that in developing...   \n",
       "2  We consider an MU-MISO wireless broadcast chan...   \n",
       "3  With the increase in the cognition of marine a...   \n",
       "4  The typical structure of a PMT is shown in Fig...   \n",
       "\n",
       "                                           citations  \n",
       "0  {'BIBREF0': {'title': 'Distributionally robust...  \n",
       "1  {'BIBREF0': {'title': 'Listening', 'authors': ...  \n",
       "2  {'BIBREF0': {'title': 'Convex optimization-bas...  \n",
       "3  {'BIBREF0': {'title': 'AUV buoyancy regulating...  \n",
       "4  {'BIBREF0': {'title': 'A statistical non-linea...  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "with open('./Dataset/text_df/pdf_parses_10.pkl', 'rb') as f:\n",
    "    batch_df = pickle.load(f)\n",
    "batch_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Boolean index has wrong length: 13715 instead of 10090",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-1ae48fda764d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mzero_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwhole_text_inds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mzero_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2125\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2127\u001b[0m         \u001b[1;31m# a list of integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1780\u001b[0m         \u001b[1;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1782\u001b[1;33m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1783\u001b[0m         \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1784\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2327\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2328\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexers.py\u001b[0m in \u001b[0;36mcheck_array_indexer\u001b[1;34m(array, indexer)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;31m# GH26658\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             raise IndexError(\n\u001b[0m\u001b[0;32m    401\u001b[0m                 \u001b[1;34mf\"Boolean index has wrong length: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"{len(indexer)} instead of {len(array)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Boolean index has wrong length: 13715 instead of 10090"
     ]
    }
   ],
   "source": [
    "zero_df = text_df.iloc[whole_text_inds]\n",
    "zero_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625 10090\n"
     ]
    }
   ],
   "source": [
    "whole_text_inds = [type(x) != str or len(x)== 0  for x in text_df['whole_text']]\n",
    "print(np.sum(whole_text_inds), text_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing index to paper_id\n",
      "0.1245725154876709\n",
      "0.1245725154876709\n",
      "0.1245725154876709\n",
      "0.1245725154876709\n",
      "0.1245725154876709\n",
      "0.1245725154876709\n",
      "Tokenizing\n",
      "53.112943172454834\n",
      "53.112943172454834\n",
      "53.112943172454834\n",
      "Lemmatizing\n",
      "53.112943172454834\n",
      "Bag of Words Representation\n",
      "length of dct before filter_extreme:  158730\n",
      "length of dct after filter_extreme:  25185\n",
      "159.25587630271912\n"
     ]
    }
   ],
   "source": [
    "docs_cleaned = clean_pdf(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4443 4443\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_cleaned['ids']), len(docs_cleaned['corpus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sections(textdf, file_name=''):\n",
    "    def merge_sections(doc):\n",
    "        sections = doc['body_text']\n",
    "        #print(len(sections))\n",
    "        #print(sections[0])\n",
    "        if type(sections) is list and len(sections)>0:\n",
    "            section_titles = [x['section'] for x in sections]\n",
    "            paper_id = doc['paper_id']\n",
    "            texts = []; titles = [];\n",
    "            current_text = sections[0]['text']\n",
    "            current_title = section_titles[0]\n",
    "            for i in range(1,len(sections)):\n",
    "                #print(i)\n",
    "                if section_titles[i] == current_title:\n",
    "                    current_text = current_text + ' ' + sections[i]['text']\n",
    "                else: \n",
    "                    texts.append(current_text)\n",
    "                    titles.append(current_title)\n",
    "                    current_title = section_titles[i]\n",
    "                    current_text = sections[i]['text']\n",
    "            sections_df = pd.DataFrame({'whole_text': texts, 'titles':titles, 'paper_id':paper_id})\n",
    "            return sections_df\n",
    "        else:\n",
    "            return []\n",
    "    total_df = pd.DataFrame(columns = ['whole_text', 'titles', 'paper_id'])\n",
    "    for index,row in textdf.iterrows():\n",
    "        #print(index)\n",
    "        \n",
    "        total_df = total_df.append(merge_sections(row))\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_df = process_sections(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whole_text</th>\n",
       "      <th>titles</th>\n",
       "      <th>paper_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Throughout the technical note, we use capital ...</td>\n",
       "      <td>II. PRELIMINARIES</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1) The confidence set O ns s is bounded and ha...</td>\n",
       "      <td>Assumption 3 (Regularity Conditions forC s ):</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This section focuses on DAMDP with a finite nu...</td>\n",
       "      <td>III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this section, we study two synthetic numeri...</td>\n",
       "      <td>IV. SIMULATION</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We consider a machine replacement problem simi...</td>\n",
       "      <td>A. Reward Uncertainty in the Machine Replaceme...</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We first consider a machine replacement proble...</td>\n",
       "      <td>1) Machine Replacement as a MDP With Gaussian ...</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The second experiment has a similar setup as t...</td>\n",
       "      <td>2) Machine Replacement as a MDP With Mixed Gau...</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We now consider a path planning problem, simil...</td>\n",
       "      <td>B. Transition Uncertainty in the Path Planning...</td>\n",
       "      <td>18980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morley (1991) has explained that in developing...</td>\n",
       "      <td>A. Listening Materials and Activities</td>\n",
       "      <td>56031008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prior knowledge in listener\"s mind entails the...</td>\n",
       "      <td>B. Schema Theory and Background Knowledge</td>\n",
       "      <td>56031008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          whole_text  \\\n",
       "0  Throughout the technical note, we use capital ...   \n",
       "1  1) The confidence set O ns s is bounded and ha...   \n",
       "2  This section focuses on DAMDP with a finite nu...   \n",
       "3  In this section, we study two synthetic numeri...   \n",
       "4  We consider a machine replacement problem simi...   \n",
       "5  We first consider a machine replacement proble...   \n",
       "6  The second experiment has a similar setup as t...   \n",
       "7  We now consider a path planning problem, simil...   \n",
       "0  Morley (1991) has explained that in developing...   \n",
       "1  Prior knowledge in listener\"s mind entails the...   \n",
       "\n",
       "                                              titles  paper_id  \n",
       "0                                  II. PRELIMINARIES  18980380  \n",
       "1      Assumption 3 (Regularity Conditions forC s ):  18980380  \n",
       "2   III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS  18980380  \n",
       "3                                     IV. SIMULATION  18980380  \n",
       "4  A. Reward Uncertainty in the Machine Replaceme...  18980380  \n",
       "5  1) Machine Replacement as a MDP With Gaussian ...  18980380  \n",
       "6  2) Machine Replacement as a MDP With Mixed Gau...  18980380  \n",
       "7  B. Transition Uncertainty in the Path Planning...  18980380  \n",
       "0              A. Listening Materials and Activities  56031008  \n",
       "1          B. Schema Theory and Background Knowledge  56031008  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import itertools \n",
    "\n",
    "def get_word_postag(word):\n",
    "    #if pos_tag([word])[0][1].startswith('J'):\n",
    "    #    return wordnet.ADJ\n",
    "    #if pos_tag([word])[0][1].startswith('V'):\n",
    "    #    return wordnet.VERB\n",
    "    if pos_tag([word])[0][1].startswith('N'):\n",
    "        #return wordnet.NOUN\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        #return wordnet.ADJ\n",
    "        #return wordnet.NOUN\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Preprocessing: tokenize words\n",
    "def tokenize(text):\n",
    "    return(word_tokenize(text))\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        return(gensim.utils.simple_preprocess(str(sentence), min_len=3,deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Preprocessing: remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords]) \n",
    "    #return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Preprocessing: lemmatizing\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Preprocessing: remove short text\n",
    "def find_longer_text(texts,k=200):\n",
    "    return list(map(lambda x: len(x.split())>k,texts))\n",
    "    \n",
    "#     lengths = list(map(lambda x: len(x.split()), texts))\n",
    "#     return [val >= k for val in lengths]\n",
    "    #return [idx for idx, val in enumerate(lengths) if val >= k] \n",
    "\n",
    "# Preprocessing: alpha num\n",
    "def keep_alphanum(words):\n",
    "    #def isalphanum(word):\n",
    "    #return word.isalnum()\n",
    "    return filter(lambda word: word.isalnum(), words)\n",
    "    #return [word for word in words if word.isalnum()]\n",
    "\n",
    "# Preprocessing: keep nouns\n",
    "def keep_nouns(words):\n",
    "    return filter(get_word_postag, words)\n",
    "    #return [word for word in words if get_word_postag(word) =='n']\n",
    "\n",
    "# Preprocessing: keep words >= 3 in length\n",
    "def keep_longer_words(words):\n",
    "    return list(filter(lambda x: (len(x) >= 3), words))\n",
    "    #return [word for word in words if len(word) >= 3]\n",
    "\n",
    "# Preprocessing: lemmatize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(words):\n",
    "    return list(map(lm.lemmatize, words)) \n",
    "\n",
    "# Preprocessing: stemming\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "def stemming(words):\n",
    "    #return [ps.stem(word) for word in words]\n",
    "    return map(ps.stem, words)\n",
    "\n",
    "def remove_digits(words):\n",
    "    return list(filter(lambda x: x.isalpha(), words))\n",
    "    #return list(filter(lambda x: x.isalpha(), words))\n",
    "#     return [word for word in words if word.isalpha()]\n",
    "\n",
    "def merged(words):\n",
    "    return ' '.join(word for word in words)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_keep(meta_df,fields):\n",
    "    pass\n",
    "def clean_section(text_df, file_name='', output_dir='',section_lvl = False):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    # if index is not paper_id\n",
    "    if text_df.index.name is None: \n",
    "        print('changing index to paper_id')\n",
    "        text_df = text_df.set_index('paper_id')\n",
    "\n",
    "    ids = text_df.index.values.astype(str)\n",
    "        \n",
    "    contents = text_df['whole_text'].values.tolist()\n",
    "    \n",
    "    # Add abstract to text\n",
    "    if not section_lvl:\n",
    "        abstracts = text_df['abstract'].values.tolist()\n",
    "\n",
    "        contents = [i + j for i, j in zip(contents, abstracts)]\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    contents = map(lambda x: re.sub('\\s+', ' ', x), contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: lower case text\n",
    "    contents = map(lambda x: x.lower(),contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: keep alphanumeric\n",
    "    contents = map(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x), contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stand along numbers\n",
    "    contents = map(lambda x: re.sub(\" \\d+ \", \" \", x), contents)\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stop words\n",
    "    contents = map(remove_stopwords, contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    contents = list(contents)\n",
    "\n",
    "    # Preprocessing: remove short text\n",
    "    inds = find_longer_text(contents)\n",
    "    contents = itertools.compress(contents, inds)\n",
    "    ids = list(itertools.compress(ids, inds))\n",
    "    \n",
    "    #key_words = text_df.loc[ids]['key_words'].values\n",
    "    print('Tokenizing')\n",
    "    \n",
    "    # Tokenize words + remove punctuation\n",
    "    tokenized_contents = map(tokenize,contents) # documents in BOW format\n",
    "#     word_list = [tokenize(article) for article in contents]\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove numbers\n",
    "    tokenized_contents = map(remove_digits, tokenized_contents)\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Keep longer words\n",
    "#     word_list = [keep_longer_words(words) for words in  word_list]\n",
    "    tokenized_contents = map(keep_longer_words,  tokenized_contents)\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Lemmatizing')\n",
    "    \n",
    "    # Preprocessing: lemmatize\n",
    "    tokenized_contents = map(lemmatize, tokenized_contents)\n",
    "    \n",
    "#     print(list(word_list)[0])\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Bag of Words Representation')\n",
    "    tokenized_contents = list(tokenized_contents)\n",
    "\n",
    "    dct = corpora.Dictionary(tokenized_contents) # make dct before corpus\n",
    "#     doc2bow = partial(dct.doc2bow,allow_update=True)\n",
    "    \n",
    "    print('length of dct before filter_extreme: ', len(dct))\n",
    "    dct.filter_extremes() # using default params # filter dct before creating corpus\n",
    "    print('length of dct after filter_extreme: ', len(dct))\n",
    "    \n",
    "\n",
    "    # Make corpus after any changes to dct\n",
    "    corpus = list(map(lambda x: dct.doc2bow(x,allow_update=True), tokenized_contents))    \n",
    "#     corpus = [dct.doc2bow(doc, allow_update=True) for doc in word_list]\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "\n",
    "\n",
    "    \n",
    "    word_list =  [item for sublist in tokenized_contents for item in sublist]\n",
    "    counter=collections.Counter(word_list)\n",
    "    \n",
    "    if file_name:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file_name = output_dir + file_name + '_clean.pkl'\n",
    "\n",
    "\n",
    "        with open(output_file_name, 'wb') as f:  # Python 3: open(..., 'wb') \n",
    "            d = {'dct': dct, 'corpus': corpus, 'docs': tokenized_contents, \n",
    "                 'counter': counter, 'ids': ids, 'word_list':word_list}\n",
    "            pickle.dump(d, f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing index to paper_id\n",
      "0.002969980239868164\n",
      "0.002969980239868164\n",
      "0.002969980239868164\n",
      "0.002969980239868164\n",
      "0.002969980239868164\n",
      "0.002969980239868164\n",
      "Tokenizing\n",
      "5.570342540740967\n",
      "5.570342540740967\n",
      "5.570342540740967\n",
      "Lemmatizing\n",
      "5.570342540740967\n",
      "Bag of Words Representation\n",
      "length of dct before filter_extreme:  31656\n",
      "length of dct after filter_extreme:  7942\n",
      "13.497344732284546\n"
     ]
    }
   ],
   "source": [
    "sections_cleaned = clean_section(sections_df, file_name='section_level_kw', output_dir='./Dataset/cleaned/cs-med/',section_lvl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights\n",
    "\n",
    "def ts_doc(dct, corpus, paper_ids):\n",
    "    paper_ids_unique = np.unique(paper_ids)\n",
    "    nwords = len(sections_cleaned['dct'].token2id)\n",
    "    weights = np.zeros((nwords, len(sections_cleaned['corpus'])))\n",
    "    for paper_id in paper_ids_unique:\n",
    "        inds = np.where(paper_ids==paper_id)\n",
    "        nonzero_words = []\n",
    "        for ind in inds:\n",
    "            nonzero_words_add = [corpus[ind][x][0] for x in range(len(corpus[ind]))]\n",
    "            nonzero_words = nonzero_words + nonzero_words_add\n",
    "        nonzero_words = np.unique(nonzero_words)\n",
    "        for \n",
    "        \n",
    "def ts_section()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = sections_cleaned['dct']\n",
    "dct.cfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x[1] for x in corpus[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sections_cleaned['corpus']\n",
    "dct = sections_cleaned['dct']\n",
    "\n",
    "weights = np.zeros(())\n",
    "for d_idx,d in enumerate(corpus):\n",
    "    for w_idx, c in enumerate(d):\n",
    "        \n",
    "        # section length\n",
    "        section_length = sum([x[1] for x in d])\n",
    "        \n",
    "        # Calculate section-level weight\n",
    "        weight_section = (c[1] + 1) / (dct.cfs[w_idx] + section_length)\n",
    "        \n",
    "        # Calculate document-level weight\n",
    "        weight_document = () / ()\n",
    "        \n",
    "        # Combined weight\n",
    "        weight_combined = weight_selection*weight_document\n",
    "    \n",
    "        weights[d_idx][w_idx] = weight_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x283bd08e160>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_cleaned['dct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_words = [sections_cleaned['corpus'][0][x][0] for x in range(len(sections_cleaned['corpus'][0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonzero_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = sections_df['paper_id']\n",
    "paper_id = paper_ids.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sections_cleaned['corpus']\n",
    "inds = np.where(paper_ids==paper_id)\n",
    "corpus_select = [ corpus[index] for index in inds[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_select = [ corpus[index] for index in inds[0] ]\n",
    "len(corpus_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 10),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 2),\n",
       " (13, 2),\n",
       " (14, 2),\n",
       " (15, 2),\n",
       " (16, 2),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 2),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 2),\n",
       " (30, 1),\n",
       " (31, 2),\n",
       " (32, 6),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 2),\n",
       " (40, 2),\n",
       " (41, 1),\n",
       " (42, 9),\n",
       " (43, 2),\n",
       " (44, 3),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 10),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 2),\n",
       " (58, 4),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 2),\n",
       " (64, 1),\n",
       " (65, 7),\n",
       " (66, 3),\n",
       " (67, 3),\n",
       " (68, 2),\n",
       " (69, 2),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 3),\n",
       " (73, 1),\n",
       " (74, 2),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 2),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 3),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 3),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 2),\n",
       " (94, 1),\n",
       " (95, 2),\n",
       " (96, 1),\n",
       " (97, 2),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 2),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 4),\n",
       " (107, 2),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 2),\n",
       " (111, 3),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 2),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 2),\n",
       " (120, 7),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 8),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 3),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 6),\n",
       " (133, 2),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 2),\n",
       " (138, 3),\n",
       " (139, 2),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 2),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 4),\n",
       " (147, 2),\n",
       " (148, 2),\n",
       " (149, 1),\n",
       " (150, 2),\n",
       " (151, 21),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 3),\n",
       " (158, 10),\n",
       " (159, 3),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 2),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 2),\n",
       " (170, 3),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 2),\n",
       " (174, 1),\n",
       " (175, 3),\n",
       " (176, 1),\n",
       " (177, 9),\n",
       " (178, 3),\n",
       " (179, 4),\n",
       " (180, 2),\n",
       " (181, 1),\n",
       " (182, 6),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 1),\n",
       " (7942, 1),\n",
       " (7943, 1),\n",
       " (7944, 4),\n",
       " (7945, 1),\n",
       " (7946, 1),\n",
       " (7947, 1),\n",
       " (7948, 1),\n",
       " (7949, 2),\n",
       " (7950, 1),\n",
       " (7951, 1),\n",
       " (7952, 1),\n",
       " (7953, 1),\n",
       " (7954, 2),\n",
       " (7955, 1),\n",
       " (7956, 1),\n",
       " (7957, 5),\n",
       " (7958, 1),\n",
       " (7959, 1),\n",
       " (7960, 1),\n",
       " (7961, 1),\n",
       " (7962, 1),\n",
       " (7963, 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate weights\n",
    "sections_cleaned['corpus'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
