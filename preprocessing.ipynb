{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/virenbajaj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/virenbajaj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/virenbajaj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import io \n",
    "import os\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from functools import partial \n",
    "import copy\n",
    "import sys\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pickle\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import itertools as it\n",
    "import re\n",
    "import time\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_pdf(file_name, start_line, end_line, ids):\n",
    "#     papers_ids_text = []; abstract = []; body_text = []; whole_text = []\n",
    "\n",
    "#     with open(file_name) as f:\n",
    "#         for _ in range(start_line):\n",
    "#             next(f)\n",
    "#         index = 0\n",
    "#         for line in f:\n",
    "#             paper = json.loads(line)\n",
    "# #             if index > end_line - start_line:\n",
    "# #                 break\n",
    "# #             index += 1\n",
    "#             if paper['paper_id'] in ids:\n",
    "#                 print('in')\n",
    "#                 papers_ids_text.append(paper['paper_id'])\n",
    "#                 if paper['abstract']:\n",
    "#                     print(paper['abstract'])\n",
    "#                     abstract.append(paper['abstract'][0]['text'])\n",
    "#                 else: \n",
    "#                     abstract.append('')\n",
    "#                 text = []\n",
    "#                 full_text = ''\n",
    "#                 if paper['body_text']:\n",
    "#                     for entry in paper['body_text']:\n",
    "#                         if entry['section'] and entry['text']:\n",
    "#                             section = {key: entry[key] for key in ['section', 'text']}\n",
    "#                             text.append(section)\n",
    "#                             if full_text:\n",
    "#                                 full_text = full_text + '\\n' + entry['text']\n",
    "#                             else:\n",
    "#                                 full_text = entry['text']\n",
    "#                     body_text.append(text)\n",
    "#                     whole_text.append(full_text)\n",
    "#                 else:\n",
    "#                     body_text.append([])\n",
    "#                     whole_text.append('')\n",
    "                \n",
    "#         textdata = pd.DataFrame({'paper_id': papers_ids_text, 'abstract': abstract, 'body_text': body_text, 'whole_text': whole_text})\n",
    "\n",
    "#         return textdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Uncompressing papers from fields we want, saving the metadata and pdf_parses as pickled dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metadata(metadata_file,fields=None, get_ids=None,output_dir = './processed/',put_in_op_dir=False):\n",
    "    \n",
    "    file_name_without_path_or_ext = metadata_file.split('/')[-1].split('.')[0]\n",
    "    # Go through metadata files to get relevant paper ids and titles\n",
    "    ids = []; title = []; field = [];\n",
    "    # if file is compressed\n",
    "    if metadata_file[-3:] == '.gz':\n",
    "        if put_in_op_dir:\n",
    "            output_file = output_dir + file_name_without_path_or_ext\n",
    "        else:  \n",
    "            output_file = metadata_file[:-3]\n",
    "        gz = gzip.open(metadata_file, 'rb')\n",
    "        f = io.BufferedReader(gz)\n",
    "        f_out = open(output_file,'wb')\n",
    "    else:\n",
    "        f = open(metadata_file)\n",
    "        f_out = None\n",
    "\n",
    "    for line in tqdm(f.readlines()):\n",
    "        paper = json.loads(line)\n",
    "        if not fields:\n",
    "            if not ids:\n",
    "                ids.append(paper['paper_id'])\n",
    "                title.append(paper['title'])\n",
    "                field.append(paper['mag_field_of_study'])\n",
    "                if f_out:\n",
    "                    f_out.write(line)\n",
    "            elif paper['paper_id'] in get_ids:\n",
    "                ids.append(paper['paper_id'])\n",
    "                title.append(paper['title'])\n",
    "                field.append(paper['mag_field_of_study'])\n",
    "                if f_out:\n",
    "                    f_out.write(line)\n",
    "        elif paper['mag_field_of_study']:\n",
    "            field_in = any([x in fields for x in paper['mag_field_of_study']])\n",
    "            if field_in:\n",
    "                ids.append(paper['paper_id'])\n",
    "                title.append(paper['title'])\n",
    "                field.append(paper['mag_field_of_study'])\n",
    "                if f_out:\n",
    "                    f_out.write(line)\n",
    "    f.close()\n",
    "    if f_out:\n",
    "        f_out.close()\n",
    "    # create and save dataframe in output_dir/meta_df\n",
    "    meta_df = pd.DataFrame({'ids':ids, 'titles':title, 'field': field})\n",
    "    meta_df_dir = output_dir + 'meta_df/'\n",
    "    os.makedirs(meta_df_dir, exist_ok=True)\n",
    "    meta_df_file = meta_df_dir + file_name_without_path_or_ext + '.pkl'\n",
    "    with open(meta_df_file, 'wb') as f:\n",
    "        pickle.dump(meta_df, f)\n",
    "    return meta_df_file\n",
    "\n",
    "\n",
    "def process_pdf(meta_df_file, pdf_file,fields=None,get_ids=None, output_dir = './processed/'):\n",
    "    \n",
    "    # use meta data df to check ids\n",
    "    with open(meta_df_file, 'rb') as f:\n",
    "        meta_df = pickle.load(f)\n",
    "    # get the pdfs \n",
    "    # lists to make pdf dataframe from \n",
    "    papers_ids_text = []; abstract = [] \n",
    "    body_text = []  # list of dicts (for each paper) with section, text, cite_spans\n",
    "    whole_text = [] # list of strings (for each paper) of entire text in the body\n",
    "    key_words = []  # key words mentioned with the abstract\n",
    "    citations = [] \n",
    "    # if file is compressed\n",
    "    if pdf_file[-3:] == '.gz':\n",
    "        output_file = pdf_file[:-3]\n",
    "        gz = gzip.open(pdf_file, 'rb')\n",
    "        f = io.BufferedReader(gz)\n",
    "        f_out = open(output_file,'wb')\n",
    "    else:\n",
    "        f = open(pdf_file)\n",
    "        f_out = None\n",
    "        \n",
    "    for line in tqdm(f.readlines()):\n",
    "        paper = json.loads(line)\n",
    "        if paper['paper_id'] in meta_df['ids'].values: #ids defined in meta data df based on field\n",
    "            if f_out: # if untaring\n",
    "                f_out.write(line) # write that pdf (untarred pdf parse will only have selected papers)\n",
    "            # get paper-id\n",
    "            papers_ids_text.append(paper['paper_id'])\n",
    "            abstract_text = ''\n",
    "            terms = []\n",
    "            # get abstract\n",
    "            if paper['abstract']:\n",
    "                abstract_text = paper['abstract'][0]['text']\n",
    "                # get key_words in abstract\n",
    "                if len(paper['abstract'])>1:   \n",
    "                    if paper['abstract'][1]['text'][:11].lower() == 'index terms':\n",
    "                        terms = paper['abstract'][1]['text'][12:].split(',') #remove \"Index Terms-\" or \"INDEX TERMS \" from string    \n",
    "            abstract.append(abstract_text) \n",
    "            key_words.append(terms)\n",
    "            # get sections and text from body\n",
    "            text = []\n",
    "            full_text = ''\n",
    "            if paper['body_text']:\n",
    "                for entry in paper['body_text']:\n",
    "                    if entry['section'] and entry['text']:\n",
    "                        section = {key: entry[key] for key in ['section', 'text', 'cite_spans']}\n",
    "                        text.append(section)\n",
    "                        if full_text: # why the if-else?\n",
    "                            full_text = full_text + '\\n' + entry['text']\n",
    "                        else:\n",
    "                            full_text = entry['text']\n",
    "            body_text.append(text)\n",
    "            whole_text.append(full_text)\n",
    "            # get citations\n",
    "            bib_entries = {}\n",
    "            if paper['bib_entries']:\n",
    "                bib_entries = paper['bib_entries']\n",
    "            citations.append(bib_entries)\n",
    "            \n",
    "    f.close()\n",
    "    if f_out:\n",
    "        f_out.close()\n",
    "    # create and save dataframe in output_dir/text_df\n",
    "    text_df = pd.DataFrame({'paper_id': papers_ids_text, 'abstract': abstract,'key_words': key_words,'body_text': body_text, 'whole_text': whole_text,'citations':citations})\n",
    "    text_df_dir = output_dir + 'text_df/'\n",
    "    os.makedirs(text_df_dir, exist_ok=True)\n",
    "    file_name_without_path_or_ext = pdf_file.split('/')[-1].split('.')[0]\n",
    "    text_df_file = text_df_dir + file_name_without_path_or_ext + '.pkl'\n",
    "    with open(text_df_file, 'wb') as f:\n",
    "        pickle.dump(text_df, f)         \n",
    "    return text_df_file       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(metadata_file,pdf_file, fields=None, ids=None, output_dir = './processed/'):\n",
    "    meta_df_file = process_metadata(metadata_file,fields,ids,output_dir)\n",
    "    text_df_file = process_pdf(meta_df_file,pdf_file,fields,ids,output_dir)\n",
    "    return meta_df_file,text_df_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Finer cleaning of data:\n",
    "- remove stop words, high-frequency words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "def get_word_postag(word):\n",
    "    #if pos_tag([word])[0][1].startswith('J'):\n",
    "    #    return wordnet.ADJ\n",
    "    #if pos_tag([word])[0][1].startswith('V'):\n",
    "    #    return wordnet.VERB\n",
    "    if pos_tag([word])[0][1].startswith('N'):\n",
    "        #return wordnet.NOUN\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        #return wordnet.ADJ\n",
    "        #return wordnet.NOUN\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Preprocessing: tokenize words\n",
    "def tokenize(text):\n",
    "    return(word_tokenize(text))\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        return(gensim.utils.simple_preprocess(str(sentence), min_len=3,deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Preprocessing: remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords]) \n",
    "    #return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Preprocessing: lemmatizing\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Preprocessing: remove short text\n",
    "def find_longer_text(texts,k=200):\n",
    "    return list(map(lambda x: len(x.split())>k,texts))\n",
    "    \n",
    "#     lengths = list(map(lambda x: len(x.split()), texts))\n",
    "#     return [val >= k for val in lengths]\n",
    "    #return [idx for idx, val in enumerate(lengths) if val >= k] \n",
    "\n",
    "# Preprocessing: alpha num\n",
    "def keep_alphanum(words):\n",
    "    #def isalphanum(word):\n",
    "    #return word.isalnum()\n",
    "    return filter(lambda word: word.isalnum(), words)\n",
    "    #return [word for word in words if word.isalnum()]\n",
    "\n",
    "# Preprocessing: keep nouns\n",
    "def keep_nouns(words):\n",
    "    return filter(get_word_postag, words)\n",
    "    #return [word for word in words if get_word_postag(word) =='n']\n",
    "\n",
    "# Preprocessing: keep words >= 3 in length\n",
    "def keep_longer_words(words):\n",
    "    return list(filter(lambda x: (len(x) >= 3), words))\n",
    "    #return [word for word in words if len(word) >= 3]\n",
    "\n",
    "# Preprocessing: lemmatize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(words):\n",
    "    return list(map(lm.lemmatize, words)) \n",
    "\n",
    "# Preprocessing: stemming\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "def stemming(words):\n",
    "    #return [ps.stem(word) for word in words]\n",
    "    return map(ps.stem, words)\n",
    "\n",
    "def remove_digits(words):\n",
    "    return list(filter(lambda x: x.isalpha(), words))\n",
    "    #return list(filter(lambda x: x.isalpha(), words))\n",
    "#     return [word for word in words if word.isalpha()]\n",
    "\n",
    "def merged(words):\n",
    "    return ' '.join(word for word in words)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sections(textdf, file_name):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_keep(meta_df,fields):\n",
    "    pass\n",
    "def clean_pdf(text_df, file_name, output_dir):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    # if index is not paper_id\n",
    "    if not text_df.index.name: \n",
    "        print('changing index to paper_id')\n",
    "        text_df.set_index('paper_id')\n",
    "        \n",
    "    ids = text_df.index.values.astype(str)\n",
    "    \n",
    "        \n",
    "    contents = text_df['whole_text'].values.tolist()\n",
    "    abstracts = text_df['abstract'].values.tolist()\n",
    "    \n",
    "    # Add abstract to text\n",
    "    contents = [i + j for i, j in zip(contents, abstracts)]\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    contents = map(lambda x: re.sub('\\s+', ' ', x), contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: lower case text\n",
    "    contents = map(lambda x: x.lower(),contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: keep alphanumeric\n",
    "    contents = map(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x), contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stand along numbers\n",
    "    contents = map(lambda x: re.sub(\" \\d+ \", \" \", x), contents)\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stop words\n",
    "    contents = map(remove_stopwords, contents)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    contents = list(contents)\n",
    "\n",
    "    # Preprocessing: remove short text\n",
    "    inds = find_longer_text(contents)\n",
    "    contents = itertools.compress(contents, inds)\n",
    "    ids = list(itertools.compress(ids, inds))\n",
    "    \n",
    "    key_words = text_df.loc[ids]['key_words'].values\n",
    "    print('Tokenizing')\n",
    "    \n",
    "    # Tokenize words + remove punctuation\n",
    "    tokenized_contents = map(tokenize,contents) # documents in BOW format\n",
    "#     word_list = [tokenize(article) for article in contents]\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove numbers\n",
    "    tokenized_contents = map(remove_digits, tokenized_contents)\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Keep longer words\n",
    "#     word_list = [keep_longer_words(words) for words in  word_list]\n",
    "    tokenized_contents = map(keep_longer_words,  tokenized_contents)\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Lemmatizing')\n",
    "    \n",
    "    # Preprocessing: lemmatize\n",
    "    tokenized_contents = map(lemmatize, tokenized_contents)\n",
    "    \n",
    "#     print(list(word_list)[0])\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Bag of Words Representation')\n",
    "    tokenized_contents = list(tokenized_contents)\n",
    "\n",
    "    dct = corpora.Dictionary(tokenized_contents) # make dct before corpus\n",
    "#     doc2bow = partial(dct.doc2bow,allow_update=True)\n",
    "    \n",
    "    print('length of dct before filter_extreme: ', len(dct))\n",
    "    dct.filter_extremes() # using default params # filter dct before creating corpus\n",
    "    print('length of dct after filter_extreme: ', len(dct))\n",
    "    \n",
    "\n",
    "    # Make corpus after any changes to dct\n",
    "    corpus = list(map(lambda x: dct.doc2bow(x,allow_update=True), tokenized_contents))    \n",
    "#     corpus = [dct.doc2bow(doc, allow_update=True) for doc in word_list]\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "\n",
    "\n",
    "    \n",
    "    word_list =  [item for sublist in tokenized_contents for item in sublist]\n",
    "    counter=collections.Counter(word_list)\n",
    "  \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_name = output_dir + file_name + '_clean.pkl'\n",
    "\n",
    "    \n",
    "    with open(output_file_name, 'wb') as f:  # Python 3: open(..., 'wb') \n",
    "        d = {'dct': dct, 'corpus': corpus, 'docs': tokenized_contents, \n",
    "             'counter': counter, 'ids': ids, 'word_list':word_list,\n",
    "             'key_words':key_words}\n",
    "        pickle.dump(d, f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_citations(text_df_file, pdf_file):\n",
    "#    'Add citation column to current DF by parsing pdf_parse'\n",
    "#     with open(text_df_file, 'rb') as f:\n",
    "#         text_df = pickle.load(f)\n",
    "        \n",
    "#     citations = []\n",
    "#     with open(pdf_file, 'rb') as f:\n",
    "#         for line in tqdm(f.readlines()):\n",
    "#             paper = json.loads(line)\n",
    "#             # get citations\n",
    "#             bib_entries = {}\n",
    "#             if paper['bib_entries']:\n",
    "#                 bib_entries = paper['bib_entries']\n",
    "#             citations.append(bib_entries)\n",
    "#     text_df['citations'] = citations\n",
    "    \n",
    "#     with open(text_df_file, 'wb') as f:\n",
    "#         pickle.dump(text_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"v\"\n",
    "if user == \"v\":\n",
    "    sample_data_dir = \"/Users/virenbajaj/Desktop/Columbia Fall 20/Graphical Models/project/20200705v1/sample/\"\n",
    "    full_data_dir = \"/Volumes/Extreme SSD/Library/SemanticScholar Data/20200705v1/full/\"\n",
    "else:\n",
    "    full_data_dir = '20200705v1/full/'\n",
    "    \n",
    "metadata_dir = full_data_dir + 'metadata/'\n",
    "pdf_parses_dir = full_data_dir + 'pdf_parses/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Volumes/Extreme SSD/Library/SemanticScholar Data/20200705v1/full/metadata/metadata_0.jsonl'] ['/Volumes/Extreme SSD/Library/SemanticScholar Data/20200705v1/full/pdf_parses/pdf_parses_0.jsonl']\n"
     ]
    }
   ],
   "source": [
    "files = range(2,99)\n",
    "metadata = [metadata_dir + f'metadata_{i}.jsonl' for i in files]\n",
    "pdfs = [pdf_parses_dir + f'pdf_parses_{i}.jsonl' for i in files]\n",
    "fields = []\n",
    "get_ids = flat_links # from cell that gets links of all refs\n",
    "print(metadata, pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534784/534784 [00:35<00:00, 14947.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# for batch in zip(metadata,pdfs):\n",
    "#     process_batch(batch[0], batch[1], ids=get_ids)\n",
    "for f in metadata:\n",
    "    process_metadata(f,fields=None, get_ids=set(get_ids),output_dir = './processed/refs/',put_in_op_dir=False)\n",
    "# process_pdf('processed/meta_df/metadata_0.pkl', pdfs[0],fields,output_dir='./processed/')\n",
    "# add_citations(text_df_file='process_pdf(meta_df_file,pdf_file,fields,output_dir)',\n",
    "#             pdf_file ='/Volumes/Extreme SSD/Library/SemanticScholar Data/20200705v1/full/pdf_parses/pdf_parses_0.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(get_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/meta_df/metadata_0.pkl', 'rb') as f:\n",
    "    meta_df = pickle.load(f)\n",
    "with open('./processed/text_df/pdf_parses_0.pkl','rb') as f:\n",
    "    text_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paper_id'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.set_index('paper_id') # SET INDEX TO PAPER ID TO INDEX INTO DF\n",
    "text_df.index.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter DF based on fields\n",
    "def field_in(df, selected):\n",
    "    'Filter df based on fields in selected'\n",
    "    def f(fields):\n",
    "        return any([x in selected for x in fields])\n",
    "    mask = list(map(f,df.field))\n",
    "    return df[mask]\n",
    "# med_meta_df = field_in(meta_df,['Medicine'])\n",
    "# cs_meta_df = field_in(meta_df, ['Computer Science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# med_text_df = text_df.filter(items=med_meta_df.ids.values, axis=0)\n",
    "# cs_text_df = text_df.filter(items=cs_meta_df.ids.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = text_df['key_words'][text_df.key_words.str.len() > 0 ] \n",
    "text_df_kw = text_df.loc[key_words.index] # papers with key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(paper_refs):\n",
    "    'Retreive the paper_ids (links) from refrences of the paper'\n",
    "    refs, links = [], []\n",
    "    for key,value in paper_refs.items():\n",
    "        if value['link']:\n",
    "            links.append(value['link'])\n",
    "            refs.append(key)\n",
    "    return links, refs\n",
    "\n",
    "all_citations = (text_df_kw.citations.values)\n",
    "all_links=[]\n",
    "all_refs=[]\n",
    "for citation in all_citations:\n",
    "    links,refs = get_links(citation)\n",
    "    all_links.append(links)\n",
    "    all_refs.append(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_df['refs'] = all_refs\n",
    "# text_df['links'] = all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_links = [item for sublist in all_links for item in sublist]\n",
    "links_found = set(flat_links).intersection(set(meta_df.ids)) # links in current df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 13982)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_found), len(flat_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5264949798583984\n",
      "0.5274271965026855\n",
      "0.5274591445922852\n",
      "0.5274801254272461\n",
      "0.5274980068206787\n",
      "0.5279140472412109\n",
      "Tokenizing\n",
      "4.992978096008301\n",
      "4.992990016937256\n",
      "4.993922233581543\n",
      "Lemmatizing\n",
      "4.994757890701294\n",
      "Bag of Words Representation\n",
      "length of dct before filter_extreme:  34400\n",
      "length of dct after filter_extreme:  6920\n",
      "16.1564302444458\n"
     ]
    }
   ],
   "source": [
    "file_name = 'pdf_parses_0_kw'\n",
    "# for field in fields:\n",
    "d = clean_pdf(text_df_kw,file_name,output_dir=f'./cleaned/cs-med/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file= './cleaned/cs-med/pdf_parses_0_kw_clean.pkl'\n",
    "with open(cleaned_file, 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = cleaned_data['dct']\n",
    "corpus = cleaned_data['corpus']\n",
    "counter = cleaned_data['counter']\n",
    "ids = list(cleaned_data['ids'])\n",
    "word_list = cleaned_data['word_list']\n",
    "docs = cleaned_data['docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n",
      "493\n",
      "34400\n",
      "34400\n",
      "493\n",
      "1279903\n"
     ]
    }
   ],
   "source": [
    "# documents = corpus.get_texts()\n",
    "print(len(corpus))\n",
    "print(len(list(ids)))\n",
    "print(len(dct.dfs))\n",
    "print(len(counter))\n",
    "print(len(docs))\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = cwd + '\\\\Preprocessed\\\\'\n",
    "# def process_pdf(file_name, batch_num, start_ind, end_ind, ids):\n",
    "#     textdf = read_pdf(file_name, start_ind, end_ind, ids)\n",
    "#     save_path = file_path + str(batchnum)\n",
    "#     output = clean_pdf(textdf, save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_batch(batch_ind, batch_size=50000, field='Computer Science'):\n",
    "#     file_name_meta = '20200705v1/full/metadata/metadata_' + str(batch_ind) + '.jsonl'\n",
    "#     file_name_pdf = '20200705v1/full/pdf_parses/pdf_parses_' + str(batch_ind) + '.jsonl'\n",
    "    \n",
    "#     import os\n",
    "#     cwd = os.getcwd()\n",
    "#     file_path = cwd + '\\\\Preprocessed\\\\' \n",
    "\n",
    "#     start = time.time()\n",
    "    \n",
    "#     nlines = sum(1 for line in open(file_name_pdf))\n",
    "#     batch_num = int(np.ceil(nlines / batch_size))\n",
    "    \n",
    "#     print('Processing metadata file', batch_ind)\n",
    "#     selected_data = process_metadata(file_name_meta, field)\n",
    "#     selected_ids = selected_data['ids'].values\n",
    "    \n",
    "#     with open(file_path+'metadata.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#         pickle.dump(selected_data, f)\n",
    "    \n",
    "#     t = time.time()\n",
    "#     print(t-start)\n",
    "    \n",
    "#     for i in range(batch_num):\n",
    "#         print('Processing pdfs batch Number: ', i)\n",
    "#         line_nums = [batch_size*i, batch_size*(i+1)]\n",
    "#         textdf = read_pdf(file_name_pdf,line_nums[0],line_nums[1], ids)  \n",
    "        \n",
    "#         t = time.time()\n",
    "#         print(t-start)\n",
    "        \n",
    "#         print('Processing pdfs batch Number: ', i)\n",
    "\n",
    "#         output = clean_pdf(textdf, file_path+str(i))\n",
    "        \n",
    "#         t = time.time()\n",
    "#         print(t-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_batch(batch_ind=0, field='Computer Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counts = sorted(dct.dfs.items(), key = lambda x: x[1], reverse=True)\n",
    "# top_ids = [x[0] for x in word_counts[0:100]]\n",
    "# top_words = [dct.id2token[x] for x in top_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dct.filter_tokens(bad_ids=top_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
