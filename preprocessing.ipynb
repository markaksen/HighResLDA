{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aksen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aksen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import io \n",
    "import os\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from functools import partial \n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pickle\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import itertools as it\n",
    "import re\n",
    "import time\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_pdf(file_name, start_line, end_line, ids):\n",
    "#     papers_ids_text = []; abstract = []; body_text = []; whole_text = []\n",
    "\n",
    "#     with open(file_name) as f:\n",
    "#         for _ in range(start_line):\n",
    "#             next(f)\n",
    "#         index = 0\n",
    "#         for line in f:\n",
    "#             paper = json.loads(line)\n",
    "# #             if index > end_line - start_line:\n",
    "# #                 break\n",
    "# #             index += 1\n",
    "#             if paper['paper_id'] in ids:\n",
    "#                 print('in')\n",
    "#                 papers_ids_text.append(paper['paper_id'])\n",
    "#                 if paper['abstract']:\n",
    "#                     print(paper['abstract'])\n",
    "#                     abstract.append(paper['abstract'][0]['text'])\n",
    "#                 else: \n",
    "#                     abstract.append('')\n",
    "#                 text = []\n",
    "#                 full_text = ''\n",
    "#                 if paper['body_text']:\n",
    "#                     for entry in paper['body_text']:\n",
    "#                         if entry['section'] and entry['text']:\n",
    "#                             section = {key: entry[key] for key in ['section', 'text']}\n",
    "#                             text.append(section)\n",
    "#                             if full_text:\n",
    "#                                 full_text = full_text + '\\n' + entry['text']\n",
    "#                             else:\n",
    "#                                 full_text = entry['text']\n",
    "#                     body_text.append(text)\n",
    "#                     whole_text.append(full_text)\n",
    "#                 else:\n",
    "#                     body_text.append([])\n",
    "#                     whole_text.append('')\n",
    "                \n",
    "#         textdata = pd.DataFrame({'paper_id': papers_ids_text, 'abstract': abstract, 'body_text': body_text, 'whole_text': whole_text})\n",
    "\n",
    "#         return textdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Uncompressing papers from fields we want, saving the metadata and pdf_parses as pickled dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(metadata_file,pdf_file, fields=None, output_dir = './processed/'):\n",
    "    # Go through metadata files to get relevant paper ids and titles\n",
    "    ids = []; title = []; \n",
    "    # if file is compressed\n",
    "    if metadata_file[-3:] == '.gz':\n",
    "        output_file = metadata_file[:-3]\n",
    "        gz = gzip.open(metadata_file, 'rb')\n",
    "        f = io.BufferedReader(gz)\n",
    "        f_out = open(output_file,'wb')\n",
    "    else:\n",
    "        f = open(metadata_file)\n",
    "        f_out = None\n",
    "\n",
    "    for line in tqdm(f.readlines()):\n",
    "        paper = json.loads(line)\n",
    "        if not fields:\n",
    "            ids.append(paper['paper_id'])\n",
    "            title.append(paper['title'])\n",
    "            if f_out:\n",
    "                f_out.write(line)\n",
    "        elif paper['mag_field_of_study']:\n",
    "            field_in = any([x in fields for x in paper['mag_field_of_study']])\n",
    "            if field_in:\n",
    "                ids.append(paper['paper_id'])\n",
    "                title.append(paper['title'])\n",
    "                if f_out:\n",
    "                    f_out.write(line)\n",
    "    f.close()\n",
    "    if f_out:\n",
    "        f_out.close()\n",
    "    # create and save dataframe in output_dir/meta_df\n",
    "    meta_df = pd.DataFrame({'ids':ids, 'titles':title})\n",
    "    meta_df_dir = output_dir + 'meta_df/'\n",
    "    os.makedirs(meta_df_dir, exist_ok=True)\n",
    "    file_name_without_path_or_ext = metadata_file.split('/')[-1].split('.')[0]\n",
    "    meta_df_file = meta_df_dir + file_name_without_path_or_ext + '.pkl'\n",
    "    with open(meta_df_file, 'wb') as f:\n",
    "        pickle.dump(meta_df, f)\n",
    "\n",
    "    # get the pdfs         \n",
    "    papers_ids_text = []; abstract = []; body_text = []; whole_text = []; key_words = [];            \n",
    "    # if file is compressed\n",
    "    if pdf_file[-3:] == '.gz':\n",
    "        output_file = pdf_file[:-3]\n",
    "        gz = gzip.open(pdf_file, 'rb')\n",
    "        f = io.BufferedReader(gz)\n",
    "        f_out = open(output_file,'wb')\n",
    "    else:\n",
    "        f = open(pdf_file)\n",
    "        f_out = None\n",
    "        \n",
    "    for line in tqdm(f.readlines()):\n",
    "        paper = json.loads(line)\n",
    "        if paper['paper_id'] in ids:\n",
    "            if f_out:\n",
    "                f_out.write(line)\n",
    "            papers_ids_text.append(paper['paper_id'])\n",
    "            abstract_text = ''\n",
    "            terms = []\n",
    "            if paper['abstract']:\n",
    "                abstract_text = paper['abstract'][0]['text']\n",
    "                if len(paper['abstract'])>1:   \n",
    "                    if paper['abstract'][1]['text'][:11].lower() == 'index terms':\n",
    "                        terms = paper['abstract'][1]['text'][12:].split(',') #remove \"Index Terms-\" or \"INDEX TERMS \" from string    \n",
    "            abstract.append(abstract_text) \n",
    "            key_words.append(terms)\n",
    "            text = []\n",
    "            full_text = ''\n",
    "            if paper['body_text']:\n",
    "                for entry in paper['body_text']:\n",
    "                    if entry['section'] and entry['text']:\n",
    "                        section = {key: entry[key] for key in ['section', 'text']}\n",
    "                        text.append(section)\n",
    "                        if full_text:\n",
    "                            full_text = full_text + '\\n' + entry['text']\n",
    "                        else:\n",
    "                            full_text = entry['text']\n",
    "            body_text.append(text)\n",
    "            whole_text.append(full_text)\n",
    "    f.close()\n",
    "    if f_out:\n",
    "        f_out.close()\n",
    "    # create and save dataframe in output_dir/text_df\n",
    "    text_df = pd.DataFrame({'paper_id': papers_ids_text, 'abstract': abstract,'key_words': key_words,'body_text': body_text, 'whole_text': whole_text})\n",
    "    text_df_dir = output_dir + 'text_df/'\n",
    "    os.makedirs(text_df_dir, exist_ok=True)\n",
    "    file_name_without_path_or_ext = pdf_file.split('/')[-1].split('.')[0]\n",
    "    text_df_file = text_df_dir + file_name_without_path_or_ext + '.pkl'\n",
    "    with open(text_df_file, 'wb') as f:\n",
    "        pickle.dump(text_df, f)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"m\"\n",
    "if user == \"v\":\n",
    "    sample_data_dir = \"/Users/virenbajaj/Desktop/Columbia Fall 20/Graphical Models/project/20200705v1/sample/\"\n",
    "    full_data_dir = \"/Volumes/Extreme SSD/Library/SemanticScholar Data/20200705v1/full/\"\n",
    "else:\n",
    "    full_data_dir = '20200705v1/full/'\n",
    "    \n",
    "metadata_dir = full_data_dir + 'metadata/'\n",
    "pdf_parses_dir = full_data_dir + 'pdf_parses/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20200705v1/full/metadata/metadata_0.jsonl'] ['20200705v1/full/pdf_parses/pdf_parses_0.jsonl']\n"
     ]
    }
   ],
   "source": [
    "files = range(1)\n",
    "metadata = [metadata_dir + f'metadata_{i}.jsonl' for i in files]\n",
    "pdfs = [pdf_parses_dir + f'pdf_parses_{i}.jsonl' for i in files]\n",
    "fields = ['Computer Science']\n",
    "print(metadata, pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 121562/121562 [00:02<00:00, 46556.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 51058/51058 [01:40<00:00, 505.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in zip(metadata,pdfs):\n",
    "    process_batch(batch[0], batch[1], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/meta_df/metadata_0.pkl', 'rb') as f:\n",
    "    meta_df = pickle.load(f)\n",
    "with open('./processed/text_df/pdf_parses_0.pkl','rb') as f:\n",
    "    text_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Finer cleaning of data:\n",
    "- remove stop words, high-frequency words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "def get_word_postag(word):\n",
    "    #if pos_tag([word])[0][1].startswith('J'):\n",
    "    #    return wordnet.ADJ\n",
    "    #if pos_tag([word])[0][1].startswith('V'):\n",
    "    #    return wordnet.VERB\n",
    "    if pos_tag([word])[0][1].startswith('N'):\n",
    "        #return wordnet.NOUN\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        #return wordnet.ADJ\n",
    "        #return wordnet.NOUN\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Preprocessing: tokenize words\n",
    "def tokenize(text):\n",
    "    return(word_tokenize(text))\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        return(gensim.utils.simple_preprocess(str(sentence), min_len=3,deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Preprocessing: remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords]) \n",
    "    #return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Preprocessing: lemmatizing\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Preprocessing: remove short text\n",
    "def find_longer_text(texts,k=200):\n",
    "    return list(map(lambda x: len(x.split())>k,texts))\n",
    "    \n",
    "#     lengths = list(map(lambda x: len(x.split()), texts))\n",
    "#     return [val >= k for val in lengths]\n",
    "    #return [idx for idx, val in enumerate(lengths) if val >= k] \n",
    "\n",
    "# Preprocessing: alpha num\n",
    "def keep_alphanum(words):\n",
    "    #def isalphanum(word):\n",
    "    #return word.isalnum()\n",
    "    return filter(lambda word: word.isalnum(), words)\n",
    "    #return [word for word in words if word.isalnum()]\n",
    "\n",
    "# Preprocessing: keep nouns\n",
    "def keep_nouns(words):\n",
    "    return filter(get_word_postag, words)\n",
    "    #return [word for word in words if get_word_postag(word) =='n']\n",
    "\n",
    "# Preprocessing: keep words >= 3 in length\n",
    "def keep_longer_words(words):\n",
    "    return filter(lambda x: (len(x) >= 3), words)\n",
    "    #return [word for word in words if len(word) >= 3]\n",
    "\n",
    "# Preprocessing: lemmatize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(words):\n",
    "    return (map(lm.lemmatize, words)) # removing list\n",
    "\n",
    "# Preprocessing: stemming\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "def stemming(words):\n",
    "    #return [ps.stem(word) for word in words]\n",
    "    return map(ps.stem, words)\n",
    "\n",
    "def remove_digits(words):\n",
    "    return filter(lambda x: x.isalpha(), words)\n",
    "    #return list(filter(lambda x: x.isalpha(), words))\n",
    "#     return [word for word in words if word.isalpha()]\n",
    "\n",
    "def merged(words):\n",
    "    return ' '.join(word for word in words)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "Codes =['C', 'C++', 'Java', 'Python'] \n",
    "Codes = map(len,Codes)\n",
    "selectors = [False, False, False, True] \n",
    "  \n",
    "Best_Programming = itertools.compress(Codes, selectors) \n",
    "# x = list(map(len,Best_Programming) )\n",
    "# print(x)\n",
    "for each in Best_Programming: \n",
    "    print(each) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pdf(text_df, file_name, output_dir='./cleaned/'):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Convert to list\n",
    "    ids = text_df['paper_id'].values.tolist()\n",
    "    contents = text_df['whole_text'].values.tolist()\n",
    "    abstracts = text_df['abstract'].values.tolist()\n",
    "    \n",
    "    # Add abstract to text\n",
    "    contents = [i + j for i, j in zip(contents, abstracts)]\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    contents = (map(lambda x: re.sub('\\s+', ' ', x), contents))\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: lower case text\n",
    "    contents = (map(lambda x: x.lower(),contents))\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: keep alphanumeric\n",
    "    contents = (map(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x), contents)) \n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stand along numbers\n",
    "    contents = (map(lambda x: re.sub(\" \\d+ \", \" \", x), contents))\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Preprocessing: remove stop words\n",
    "    contents = (map(remove_stopwords, contents))\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    contents = list(contents)\n",
    "    \n",
    "    # Preprocessing: remove short text\n",
    "    inds = find_longer_text(contents)\n",
    "    contents = (itertools.compress(contents, inds))\n",
    "    ids = (itertools.compress(ids, inds))\n",
    "    \n",
    "    print(list(contents)[0])\n",
    "    \n",
    "    print('Tokenizing')\n",
    "    \n",
    "    # Tokenize words + remove punctuation\n",
    "    word_list = (map(tokenize,contents))\n",
    "#     word_list = [tokenize(article) for article in contents]\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Remove numbers\n",
    "    word_list = (map(remove_digits, word_list))\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    # Keep longer words\n",
    "#     word_list = [keep_longer_words(words) for words in  word_list]\n",
    "    word_list = map(keep_longer_words,  word_list)\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Lemmatizing')\n",
    "    \n",
    "    # Preprocessing: lemmatize\n",
    "    word_list = (map(lemmatize, word_list))\n",
    "    \n",
    "    print(list(word_list)[0])\n",
    "    \n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    print('Bag of Words Representation')\n",
    "    # Preprocessing: \n",
    "    dct = corpora.Dictionary()\n",
    "    doc2bow = partial(dct.doc2bow,allow_update=True)\n",
    "    corpus = map(doc2bow, word_list)\n",
    "#     corpus = [dct.doc2bow(doc, allow_update=True) for doc in word_list]\n",
    "\n",
    "    t = time.time()\n",
    "    print(t-start)\n",
    "    \n",
    "    #dct.save(file_name+'.dict')\n",
    "    word_list = list(word_list)\n",
    "    word_list =  [item for sublist in word_list for item in sublist]\n",
    "    counter=collections.Counter(word_list)\n",
    "    print(type(corpus))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_name = output_dir + file_name + '_clean.pkl'\n",
    "    with open(output_file_name, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump({'dct': dct, 'corpus': list(corpus), 'counter': counter,'ids': ids, 'word_list': word_list}, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003989219665527344\n",
      "0.003989219665527344\n",
      "0.003989219665527344\n",
      "0.003989219665527344\n",
      "0.003989219665527344\n",
      "0.003989219665527344\n",
      "throughout technical note use capital letters denote matrices bold face letters denote column vectors use e denote ith elementary vector length use r n denote nonnegative orthant r n c set joint probability distributions three random vectors b c ab c denotes set marginal distributions b use represent mixture distribution given two probability distributions f f bernoulli random variable x takes value wp p xf xf random variable follows distribution f wp p follows f wp p use n represent gaussian distribution mean variance finite markov decision process mdp defined 6tuple p r possibly infinite decision horizon ieee personal use permitted republicationredistribution requires ieee permission see httpwwwieeeorgpublicationsstandardspublicationsrightsindexhtml information 1 discount factor state set action set state assumed finite parameter p r transition probability expected reward respectively rs expected reward ps probability next state following denote set historydependent randomized strategies hr use subscript denote value associated state eg r denotes vector form rewards associated state randomized action chosen state strategy elements vector p listed following way transition probabilities action arranged block inside block listed according order next state use denote random state following denote probability simplex use represent cartesian product eg p ss p given strategy hr denote expected discounted totalreward parameters pair p r u p r distributionally ambiguous mdp damdp defined tuple ac transition probability p expected reward r unknown instead assumed obey joint distribution also unknown belongs known ambiguity set damdp framework general mostc result formulations computationally intractable eg hence make following requirement ofc parameters among different states independent assumption ambiguity setc following property statewise ambiguity setc set distributions parameters state definition ofc statewise property applies c well property concept srectangularity essential reducing damdp robust mdp lemma addition showed robust mdp coupled uncertainty sets computationally challenging implies solving damdp nonrectangular ambiguity sets even harder discuss admissible statewise ambiguity set formulation statewise ambiguity set follows unifying framework specific given statewise ambiguity set representable following standard form lower upper bounds probability parameters belong confidence set thus confidence set provides estimation uncertain parameters pair p r subject different confidence level ambiguity setsc contain prescribed conic representable confidence sets mean values residing affine manifold rich enough encompass extend several ambiguity sets considered recent literature eg set joint distribution p r hence c ps rsc notice classical technique called lifting used introduce auxiliary random vector nonlinear relationship modeled linearly example constraint variance modeled using standard form see example otherwise impossible without auxiliary variable lifting technique thus allows us model rich variety structural information marginal distribution p r unified manner note ambiguity set contains support random variables ie apriori information unknown parameters belong uncertainty set assumptions standard requirements confidence sets proposed first one asserts relationship different confidence sets nesting condition illustrated fig 1b next require thatc satisfies following regularity condition confidence set ns bounded probability one proper cones ie closed convex pointed cone nonempty interior section focuses damdp finite number decision stages show strategy defined backward induction call srobust strategy distributionally robust show strategy solvable polynomial time mild technical conditions generalizes results significantly general class ambiguity sets similar assume state visited multiple times time take different parameter realization nonstationary model assumption justified mainly stationary model generally intractable lowerbound given nonstationary model therefore multiple visits state treated visiting different states introducing dummy states assumption finite horizon damdp make following assumption without loss generality simplify exposition assumption 1 state belongs one stage terminal reward equals zero first stage contains one state ini using condition assumption partition according stage state belongs let set states belong tth stage hr c denote expected performance damdp w ini e pr u p r u p rdp r words strategy evaluated expected performance respective adversarial distribution uncertain parameters distributionally robust strategy optimal strategy according metric main focus section deriving approaches solve distributionally robust strategy end need following definition definition given damdp ac define srobust strategy follows strategy srobust strategy every history h ends conditioned history h srobust action definition requires strategy must robust wrt subproblem hence name srobust following theorem shows srobust strategy distributionally robust main result technical note theorem let assumptions 2 srobust strategy distributionally robust strategy respect c exists c saddle point proof first state lemma lemma without proof lemma assumption fix hr c denote p e p r e r w ini u p r lemma means strategy expected performance admissible distribution depends expected value parameters thus distributionally robust mdps reduce robust mdps next characterize set expected value parameters lemma define set z e p r c set z convex compact proof first show set defined asz e p r c convex compact convexity easily shown omitted due space constraints see details show compactness notice thatc weakly closed ie closed wrt weak topology since feasible set constraint weakly closed implies intersection also weakly closed thusz closed since image ofc expectation continuous function impliesz compact since ns bounded hencez bounded finally since z projection onto first two coordinates set z convexity compactness thus follow lemma implies exists p r z satisfies inf psrszs u p r u p r since saddle point minimax objective exists robust mdps eg complete proof part following similar procedure last portion proof theorem omit details due space constraint see details part follows part immediately investigate computational aspect finding srobust action theorem assumption 3 srobust action optimal solution following optimization problem termed srobust problem hereafter proof proof essentially follows duality convex optimization found longer version technical note thus since compact solve srobust action polynomial time k easy cones linear conic quadratic semidefinite cones moreover using theorem backward induction obtain srobust strategy efficiently virtue lifting technique theorem show several widely used ambiguity sets indeed special cases ofc defined derive corresponding srobust problems see additional examples variance expected huber loss function example also treated via classical robust optimization virtue lemma finite horizon damdp easily extended discountedreward infinite horizon setup generalize notion srobust strategy turns distributionally robust stationary nonstationary models extension similar found section study two synthetic numerical examples machine replacement problem path planning problem machine replacement problem reward parameters uncertain whereas path planning problem transition probabilities uncertain results generated desktop intel core i53570 cpu ghz clock speed gb ram srobust problems solved matlab using cvx package consider machine replacement problem similar one consider repair cost incurred factory holds large number machines given machines modeled underlying mdp rewards subject uncertainty first consider machine replacement problem states actions repair repair state deterministic transitions discount factor uncertain rewards following gaussian distributions independently see fig 2a first states repair action cost n 1 49th 50th states machines life designed risky repairing state incurs highly uncertain cost n 800 repairing states secure still uncertain option cost n 10 detailed implementation follows use mean value uncertain rewards compute nominal strategy robust distributionally robust strategy construct confidence sets usin first states andm state wherem mean variance estimated samples see details risky thus hard estimate addition construct extra confidence set centered mean confidence level ie 50 50 distributionally robust strategy optimal paths followed three strategies shown fig 2a performance strategies obtained using nominal robust distributionally robust approaches presented fig corresponding average total discounted rewards computational times shown table nominal strategy results highest average total discounted rewards well expected using exact mean value reward nominal fig two instances machine replacement problem fig 2a shows gaussian uncertainty rewards fig 2b shows mixed gaussian uncertainty rewards parameter however nominal strategy highly risky cannot prevent bad performance eg happening undesirable nominal strategy blind form risk finds advantage ever repairing robust strategy ends following highly conservative policy repairing machine state avoid state contrast distributionally robust optimal strategy makes use distributional information handles risk efficiently waiting state repair machine therefore strategy beats nominal robust strategies strikes good tradeoff high mean reward low variance different trials results coincide one would typically expect three solution concepts fig illustration confidence sets two distributionally robust strategies second experiment similar setup previous one except repairing 50th state reward follows mixed gaussian distribution see fig 2b experiment illustrates effect two different nestedset structures shown fig specific apply two different distributionally robust approaches proposed technical note respectively show method outperforms detailed implementation follows robust two distributionally robust strategies construct uncertainty set corresponding probability support rewards first states 50th state risky using estimated mean variance see details first distributionally robust strategy proposed construct two additional nested confidence sets 50 50 see fig 4a wp respectively uncertain rewards belong contrast second distributionally robust strategy proposed technical note construct two disjoint confidence sets 50 50 see fig b confidence level respectively specifically select two intervals around peaks two gaussian elements ie n 10 n 2 better model mixed distribution optimal paths followed three strategies shown fig 2b performance three strategies obtained presented fig corresponding average total discounted rewards computational times shown table ii expected robust strategy ends following highly conservative policy repairing machine state avoid state first distributionally robust strategy modeling mixture gaussian distribution well finds advantageous repair 50th state contrast capable capturing distribution information flexible way second distributionally robust strategy better models uncertainty finds repairing machine state optimal performance comparison clearly shows second distributionally robust strategy desirable highlights distributionally robust approach general structure confidence sets beneficial practice remark practice one obtain modality structure uncertain parameters datadriven way applying clustering algorithms initial primitive data set example one may check histogram historical observations data concentrates several distinct disjoint bins multimodel damdp approach applied moreover note networked control systems ncss recently emerged topic significant interest control community typical application ncss modern proposed novel twolayer structure solve setpoints compensation problem industrial processes networkbased environment consider path planning problem similar one presented agent wants exit maze shown fig 6a using least possible time starting upperleft corner agent move left right exit grid lowerright corner white box stands normal place agent needs one time unit pass shaded box represents shaky place agent reaches shaky place may risk jumping starting point reboot true transition probability jump follows distribution four approaches implemented follows nominal approach neglects random jump robust approach takes worstcase analysis ie assumes whole probability support transition agent jump spot highest costtogo first distributionally robust approach takes account additional information using two nested confidence sets jump probability parameter belonging confidence second distributionally robust approach proposed technical note incorporates information specific construct extra confidence interval disjoint interval states chance jumping probability performance strategies nominal robust two distributionally robust approaches shown fig 6b error bars show standard error expected time exit cpu times computing optimal policies four strategies 549 seconds respectively second distributionally robust approach achieves best performance virtually whole spectrum well expected since additional probabilistic fig 6a illustrates maze path plawnning problem fig 6b shows performance comparisons nominal robust two distributionally robust strategies runs path planning problem information available incorporated second distributionally robust approach considers ambiguity sets general structures technical note considered markov decision problems uncertainty specifically generalized distributionally robust approach proposed incorporate general ambiguity sets proposed model apriori probabilistic information uncertain parameters proposed way compute distributionally robust strategy bellman type backward induction showed strategy achieves maximum expected utility worst admissible distributions uncertain parameters solved polynomial time mild technical conditions believe many important problems usually addressed using standard mdp models could revisited better resolved using proposed models parameter uncertainty exists formulation naturally enables decision maker account general parameter uncertaintythis technical note studies markov decision processes parameter uncertainty adapt distributionally robust optimization framework assume uncertain parameters random variables following unknown distribution seek strategy maximizes expected performance adversarial distribution particular generalize previous study concentrates distribution sets special structure considerably generic class distribution sets show optimal strategy obtained efficiently mild technical conditions significantly extends applicability distributionally robust mdps incorporating probabilistic information uncertainty flexible way\n",
      "Tokenizing\n",
      "3.545159339904785\n",
      "3.545159339904785\n",
      "3.545159339904785\n",
      "Lemmatizing\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-8315197a60eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pdf_parses_0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclean_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-101-36d6ece9eb8b>\u001b[0m in \u001b[0;36mclean_pdf\u001b[1;34m(text_df, file_name, output_dir)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "file_name = 'pdf_parses_0'\n",
    "clean_pdf(text_df[0:1000],file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file= './cleaned/pdf_parses_0_clean.pkl'\n",
    "with open(cleaned_file, 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['word_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sections(textdf, file_name):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>whole_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18980380</td>\n",
       "      <td>This technical note studies Markov decision pr...</td>\n",
       "      <td>[{'section': 'II. PRELIMINARIES', 'text': 'Thr...</td>\n",
       "      <td>Throughout the technical note, we use capital ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18981111</td>\n",
       "      <td></td>\n",
       "      <td>[{'section': 'Exploration of Unknown Spaces by...</td>\n",
       "      <td>ORLY LAHAV DAVID MIODUSER Tel Aviv University,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18981625</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18982496</td>\n",
       "      <td>In this paper I discuss some constraints and i...</td>\n",
       "      <td>[{'section': 'Lack of Cooperation from Fellow ...</td>\n",
       "      <td>We normally take precautionary measures agains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18983082</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                           abstract  \\\n",
       "0  18980380  This technical note studies Markov decision pr...   \n",
       "1  18981111                                                      \n",
       "2  18981625                                                      \n",
       "3  18982496  In this paper I discuss some constraints and i...   \n",
       "4  18983082                                                      \n",
       "\n",
       "                                           body_text  \\\n",
       "0  [{'section': 'II. PRELIMINARIES', 'text': 'Thr...   \n",
       "1  [{'section': 'Exploration of Unknown Spaces by...   \n",
       "2                                                 []   \n",
       "3  [{'section': 'Lack of Cooperation from Fellow ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                          whole_text  \n",
       "0  Throughout the technical note, we use capital ...  \n",
       "1  ORLY LAHAV DAVID MIODUSER Tel Aviv University,...  \n",
       "2                                                     \n",
       "3  We normally take precautionary measures agains...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ids = selected_data['ids'].values\n",
    "# pdf_file = pdf_parses_dir + 'pdf_parses_0.jsonl'\n",
    "# textdf = read_pdf(pdf_file,0,100000, ids)\n",
    "# textdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sections = textdf['body_text'][0]\n",
    "# section_titles = [x['section'] for x in sections]\n",
    "# section_titles\n",
    "# sections[0]\n",
    "# sections[1]['section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Lemmatizing\n",
      "Bag of Words Representation\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# cwd = os.getcwd()\n",
    "# file_path = cwd + '\\\\Preprocessed\\\\0'\n",
    "# output = clean_pdf(textdf, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = output[0]\n",
    "corpus = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = cleaned_data[2]\n",
    "word_list =  [item for sublist in word_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-bf50460a09c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#wordcloud.generate(long_string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#wordcloud.generate(word_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Visualize the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counter' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "#long_string = ','.join(list(papers['paper_text_processed'].values))\n",
    "long_string = ' '.join(word_list)\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "#wordcloud.generate(long_string)\n",
    "#wordcloud.generate(word_list)\n",
    "wordcloud.generate_from_frequencies(counter)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = cwd + '\\\\Preprocessed\\\\'\n",
    "# def process_pdf(file_name, batch_num, start_ind, end_ind, ids):\n",
    "#     textdf = read_pdf(file_name, start_ind, end_ind, ids)\n",
    "#     save_path = file_path + str(batchnum)\n",
    "#     output = clean_pdf(textdf, save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_batch(batch_ind, batch_size=50000, field='Computer Science'):\n",
    "#     file_name_meta = '20200705v1/full/metadata/metadata_' + str(batch_ind) + '.jsonl'\n",
    "#     file_name_pdf = '20200705v1/full/pdf_parses/pdf_parses_' + str(batch_ind) + '.jsonl'\n",
    "    \n",
    "#     import os\n",
    "#     cwd = os.getcwd()\n",
    "#     file_path = cwd + '\\\\Preprocessed\\\\' \n",
    "\n",
    "#     start = time.time()\n",
    "    \n",
    "#     nlines = sum(1 for line in open(file_name_pdf))\n",
    "#     batch_num = int(np.ceil(nlines / batch_size))\n",
    "    \n",
    "#     print('Processing metadata file', batch_ind)\n",
    "#     selected_data = process_metadata(file_name_meta, field)\n",
    "#     selected_ids = selected_data['ids'].values\n",
    "    \n",
    "#     with open(file_path+'metadata.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#         pickle.dump(selected_data, f)\n",
    "    \n",
    "#     t = time.time()\n",
    "#     print(t-start)\n",
    "    \n",
    "#     for i in range(batch_num):\n",
    "#         print('Processing pdfs batch Number: ', i)\n",
    "#         line_nums = [batch_size*i, batch_size*(i+1)]\n",
    "#         textdf = read_pdf(file_name_pdf,line_nums[0],line_nums[1], ids)  \n",
    "        \n",
    "#         t = time.time()\n",
    "#         print(t-start)\n",
    "        \n",
    "#         print('Processing pdfs batch Number: ', i)\n",
    "\n",
    "#         output = clean_pdf(textdf, file_path+str(i))\n",
    "        \n",
    "#         t = time.time()\n",
    "#         print(t-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing metadata file 0\n",
      "35.497846364974976\n",
      "Processing pdfs batch Number:  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-39af9d441e12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocess_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Computer Science'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-287ae6b34805>\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(batch_ind, batch_size, field)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processing pdfs batch Number: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mline_nums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtextdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'20200705v1/full/pdf_parses/pdf_parses'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.jsonl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline_nums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline_nums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# process_batch(batch_ind=0, field='Computer Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "980",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-0e0278214d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtop_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-102-0e0278214d90>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtop_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 980"
     ]
    }
   ],
   "source": [
    "word_counts = sorted(dct.dfs.items(), key = lambda x: x[1], reverse=True)\n",
    "top_ids = [x[0] for x in word_counts[0:100]]\n",
    "top_words = [dct.id2token[x] for x in top_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.filter_tokens(bad_ids=top_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get_texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-8a66c0ec1b57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get_texts'"
     ]
    }
   ],
   "source": [
    "documents = corpus.get_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the LDA model\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.test.utils import common_corpus\n",
    "\n",
    "#perplexity_logger = PerplexityMetric(corpus=common_corpus, logger='shell')\n",
    "#convergence_logger = ConvergenceMetric(logger='shell')\n",
    "#coherence_cv_logger = CoherenceMetric(corpus=corpus, logger='shell', coherence = 'c_v', texts = documents)\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=2020,\n",
    "                         num_topics=10,\n",
    "                         passes=1,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -0.6558824017161562\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -0.6559.\n",
      "[([(0.005903978, 'model'),\n",
      "   (0.0056817126, 'data'),\n",
      "   (0.0050683604, 'system'),\n",
      "   (0.004683143, 'one'),\n",
      "   (0.004589382, 'set'),\n",
      "   (0.004308348, 'result'),\n",
      "   (0.004308256, 'time'),\n",
      "   (0.0041383095, 'used'),\n",
      "   (0.0041151457, 'using'),\n",
      "   (0.004082712, 'algorithm'),\n",
      "   (0.003972761, 'two'),\n",
      "   (0.0039018418, 'method'),\n",
      "   (0.0038599866, 'also'),\n",
      "   (0.0038234273, 'number'),\n",
      "   (0.0033649916, 'value'),\n",
      "   (0.0032020246, 'problem'),\n",
      "   (0.0031827844, 'case'),\n",
      "   (0.003104415, 'function'),\n",
      "   (0.0029815945, 'use'),\n",
      "   (0.0029531042, 'different')],\n",
      "  -0.15817686393398864),\n",
      " ([(0.00148182, 'method'),\n",
      "   (0.0013913988, 'ship'),\n",
      "   (0.001329745, 'model'),\n",
      "   (0.0010342445, 'set'),\n",
      "   (0.001012543, 'user'),\n",
      "   (0.0009968651, 'data'),\n",
      "   (0.0008815918, 'one'),\n",
      "   (0.00087165734, 'system'),\n",
      "   (0.0008399095, 'using'),\n",
      "   (0.00083453057, 'used'),\n",
      "   (0.000791407, 'function'),\n",
      "   (0.0007810815, 'algorithm'),\n",
      "   (0.00077615393, 'value'),\n",
      "   (0.0007526579, 'problem'),\n",
      "   (0.0007092378, 'number'),\n",
      "   (0.00070608774, 'time'),\n",
      "   (0.0006937634, 'different'),\n",
      "   (0.00069175614, 'case'),\n",
      "   (0.00068830495, 'result'),\n",
      "   (0.00065999624, 'use')],\n",
      "  -0.19138922816613405),\n",
      " ([(0.0027968646, 'data'),\n",
      "   (0.0021357208, 'set'),\n",
      "   (0.0016082124, 'using'),\n",
      "   (0.0015638896, 'two'),\n",
      "   (0.0014477217, 'used'),\n",
      "   (0.0013985019, 'use'),\n",
      "   (0.0013956556, 'one'),\n",
      "   (0.0013592504, 'system'),\n",
      "   (0.001345978, 'number'),\n",
      "   (0.0012952733, 'image'),\n",
      "   (0.0012944619, 'time'),\n",
      "   (0.0012798801, 'value'),\n",
      "   (0.0011609592, 'model'),\n",
      "   (0.001151968, 'case'),\n",
      "   (0.0010108142, 'also'),\n",
      "   (0.0009908113, 'method'),\n",
      "   (0.0009773144, 'based'),\n",
      "   (0.000935779, 'result'),\n",
      "   (0.00089157757, 'figure'),\n",
      "   (0.0008909335, 'function')],\n",
      "  -0.22409184986313432),\n",
      " ([(0.0021165882, 'needle'),\n",
      "   (0.0019584747, 'algorithm'),\n",
      "   (0.0016796219, 'data'),\n",
      "   (0.0016376894, 'system'),\n",
      "   (0.0016126306, 'used'),\n",
      "   (0.0014627527, 'number'),\n",
      "   (0.0014033306, 'using'),\n",
      "   (0.0013259042, 'time'),\n",
      "   (0.0013150312, 'method'),\n",
      "   (0.0012832065, 'node'),\n",
      "   (0.001239575, 'model'),\n",
      "   (0.0011901379, 'case'),\n",
      "   (0.0011605163, 'set'),\n",
      "   (0.0011476235, 'also'),\n",
      "   (0.0010834641, 'one'),\n",
      "   (0.0010529901, 'result'),\n",
      "   (0.0009916686, 'different'),\n",
      "   (0.0009762293, 'value'),\n",
      "   (0.00095746265, 'matrix'),\n",
      "   (0.00091953826, 'two')],\n",
      "  -0.25134008979311917),\n",
      " ([(0.023662955, 'image'),\n",
      "   (0.0055000572, 'method'),\n",
      "   (0.0045116665, 'data'),\n",
      "   (0.003928035, 'used'),\n",
      "   (0.0037993668, 'feature'),\n",
      "   (0.0032171821, 'model'),\n",
      "   (0.0031706023, 'segmentation'),\n",
      "   (0.0031630958, 'result'),\n",
      "   (0.002553997, 'pixel'),\n",
      "   (0.0024299114, 'network'),\n",
      "   (0.0024149008, 'proposed'),\n",
      "   (0.0023470058, 'using'),\n",
      "   (0.0021702317, 'different'),\n",
      "   (0.002168456, 'training'),\n",
      "   (0.0021552993, 'based'),\n",
      "   (0.0020933722, 'structure'),\n",
      "   (0.0019312912, 'color'),\n",
      "   (0.0019009954, 'two'),\n",
      "   (0.0018529977, 'information'),\n",
      "   (0.0017923183, 'figure')],\n",
      "  -0.5579335765643996),\n",
      " ([(0.0023184768, 'model'),\n",
      "   (0.002227875, 'node'),\n",
      "   (0.0020038085, 'cluster'),\n",
      "   (0.001924908, 'two'),\n",
      "   (0.0018726464, 'time'),\n",
      "   (0.0018558607, 'also'),\n",
      "   (0.001767826, 'used'),\n",
      "   (0.0015980387, 'using'),\n",
      "   (0.0014667639, 'figure'),\n",
      "   (0.0014604741, 'method'),\n",
      "   (0.0014415247, 'result'),\n",
      "   (0.0013196376, 'one'),\n",
      "   (0.0012870783, 'network'),\n",
      "   (0.0012687208, 'set'),\n",
      "   (0.0012555473, 'system'),\n",
      "   (0.0012008026, 'first'),\n",
      "   (0.0011834344, 'process'),\n",
      "   (0.0011362268, 'section'),\n",
      "   (0.0011007574, 'cell'),\n",
      "   (0.0010918492, 'mass')],\n",
      "  -0.5746132102299181),\n",
      " ([(0.026017405, 'node'),\n",
      "   (0.012148737, 'network'),\n",
      "   (0.0073822034, 'packet'),\n",
      "   (0.0060198638, 'protocol'),\n",
      "   (0.0055011758, 'number'),\n",
      "   (0.004493956, 'routing'),\n",
      "   (0.0044550938, 'time'),\n",
      "   (0.004316476, 'data'),\n",
      "   (0.004093516, 'sensor'),\n",
      "   (0.003979899, 'message'),\n",
      "   (0.0037014952, 'algorithm'),\n",
      "   (0.0036520546, 'energy'),\n",
      "   (0.003384935, 'one'),\n",
      "   (0.00335106, 'transmission'),\n",
      "   (0.003304123, 'scheme'),\n",
      "   (0.0032496708, 'path'),\n",
      "   (0.00309392, 'link'),\n",
      "   (0.0029847038, 'using'),\n",
      "   (0.002871374, 'cluster'),\n",
      "   (0.0028386982, 'based')],\n",
      "  -0.8143271300692008),\n",
      " ([(0.0031040343, 'data'),\n",
      "   (0.0015278966, 'model'),\n",
      "   (0.001497044, 'verb'),\n",
      "   (0.001434914, 'system'),\n",
      "   (0.0013891332, 'information'),\n",
      "   (0.0013804802, 'method'),\n",
      "   (0.0013726277, 'one'),\n",
      "   (0.0013565171, 'used'),\n",
      "   (0.0013548307, 'set'),\n",
      "   (0.0013339281, 'result'),\n",
      "   (0.0013248938, 'monad'),\n",
      "   (0.0013196635, 'two'),\n",
      "   (0.0012487762, 'number'),\n",
      "   (0.0012140084, 'time'),\n",
      "   (0.0011246407, 'function'),\n",
      "   (0.0010442164, 'problem'),\n",
      "   (0.0010013905, 'kleisli'),\n",
      "   (0.0009597435, 'analysis'),\n",
      "   (0.0009456633, 'show'),\n",
      "   (0.0009326994, 'case')],\n",
      "  -1.1506353201954824),\n",
      " ([(0.003437029, 'model'),\n",
      "   (0.002060297, 'set'),\n",
      "   (0.0019958434, 'used'),\n",
      "   (0.001933589, 'seller'),\n",
      "   (0.0017931446, 'using'),\n",
      "   (0.0017459083, 'buyer'),\n",
      "   (0.0017265596, 'redescription'),\n",
      "   (0.0017150694, 'two'),\n",
      "   (0.0017139321, 'group'),\n",
      "   (0.0016999695, 'graph'),\n",
      "   (0.0016761238, 'also'),\n",
      "   (0.0016577935, 'locus'),\n",
      "   (0.0016530857, 'time'),\n",
      "   (0.0016020335, 'data'),\n",
      "   (0.001595676, 'one'),\n",
      "   (0.0015612551, 'algorithm'),\n",
      "   (0.0015298325, 'use'),\n",
      "   (0.0014662513, 'number'),\n",
      "   (0.001444491, 'function'),\n",
      "   (0.0013954344, 'value')],\n",
      "  -1.21794680310391),\n",
      " ([(0.00347038, 'retailer'),\n",
      "   (0.001924207, 'rubber'),\n",
      "   (0.0018543763, 'one'),\n",
      "   (0.0017085216, 'result'),\n",
      "   (0.0016629796, 'manufacturer'),\n",
      "   (0.0016348023, 'plantation'),\n",
      "   (0.0015873768, 'time'),\n",
      "   (0.0015790883, 'used'),\n",
      "   (0.0015734532, 'data'),\n",
      "   (0.0015045947, 'figure'),\n",
      "   (0.0014655072, 'image'),\n",
      "   (0.0013718479, 'two'),\n",
      "   (0.0013211813, 'system'),\n",
      "   (0.0012562802, 'also'),\n",
      "   (0.0012319752, 'process'),\n",
      "   (0.0012304898, 'function'),\n",
      "   (0.001217165, 'study'),\n",
      "   (0.0012150613, 'using'),\n",
      "   (0.0011828527, 'bird'),\n",
      "   (0.0011572706, 'type')],\n",
      "  -1.4183699452422742)]\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "top_topics = lda_model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
